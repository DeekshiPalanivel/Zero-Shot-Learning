{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpaoUaUaPSLX"
      },
      "source": [
        "Installing packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zkBYPU9TPMGS",
        "outputId": "d5cea9c3-8e28-4a13-ba66-c5427142ec8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.1/363.4 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\n",
            "    unknown package:\n",
            "        Expected sha256 2fc8da60df463fdefa81e323eef2e36489e1c94335b5358bcb38360adf75ac9b\n",
            "             Got        556fe8a2cb5be4f5c99a0df58ff23fdf34ebbb33b7cb198f65e86cdfcc0bde4c\n",
            "\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision sentence-transformers matplotlib seaborn --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z06Hdc2pPZvN"
      },
      "source": [
        "Import statements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsISNSRlPcVm",
        "outputId": "5cccbe5d-ac44-45d3-d2e7-c4ea9a41f739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision.datasets import OxfordIIITPet\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set reproducibility\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6a73r9GRZ_Y"
      },
      "source": [
        "Defining basic image transforms and load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIW2S1zJRm03",
        "outputId": "8e0c728e-8e1a-4ead-e110-8fdf922c9bb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total examples: 3680\n",
            "Example shape: torch.Size([3, 224, 224]) | Label: 0 | Class-name: Abyssinian\n",
            "\n",
            "All 37 classes (pet breeds):\n",
            "['Abyssinian', 'American Bulldog', 'American Pit Bull Terrier', 'Basset Hound', 'Beagle', 'Bengal', 'Birman', 'Bombay', 'Boxer', 'British Shorthair', 'Chihuahua', 'Egyptian Mau', 'English Cocker Spaniel', 'English Setter', 'German Shorthaired', 'Great Pyrenees', 'Havanese', 'Japanese Chin', 'Keeshond', 'Leonberger', 'Maine Coon', 'Miniature Pinscher', 'Newfoundland', 'Persian', 'Pomeranian', 'Pug', 'Ragdoll', 'Russian Blue', 'Saint Bernard', 'Samoyed', 'Scottish Terrier', 'Shiba Inu', 'Siamese', 'Sphynx', 'Staffordshire Bull Terrier', 'Wheaten Terrier', 'Yorkshire Terrier']\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),   # ResNet-18 expects 224×224\n",
        "    transforms.ToTensor(),\n",
        "    # normalize with ImageNet means/stds\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    ),\n",
        "])\n",
        "\n",
        "# Download / load OxfordIIITPet\n",
        "dataset = OxfordIIITPet(\n",
        "    root='.',\n",
        "    download=True,\n",
        "    transform=transform,\n",
        "    target_types='category'   # returns (image, int_label)\n",
        ")\n",
        "\n",
        "print(\"Total examples:\", len(dataset))\n",
        "print(\"Example shape:\", dataset[0][0].shape, \"| Label:\", dataset[0][1], \"| Class-name:\", dataset.classes[dataset[0][1]])\n",
        "\n",
        "# Print all class names\n",
        "print(\"\\nAll {} classes (pet breeds):\".format(len(dataset.classes)))\n",
        "print(dataset.classes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzLImZnFRwDG"
      },
      "source": [
        "Split class names into seen and unseen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W09AoF7ORjpC",
        "outputId": "fba28403-32d0-4854-bb2e-cfbc13d7f5ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seen classes (22): ['Bengal', 'Maine Coon', 'English Cocker Spaniel', 'British Shorthair', 'Newfoundland', 'Ragdoll', 'Russian Blue', 'Beagle', 'Pomeranian', 'Samoyed', 'Sphynx', 'Shiba Inu', 'Siamese', 'Chihuahua', 'Egyptian Mau', 'Leonberger', 'Saint Bernard', 'Havanese', 'Yorkshire Terrier', 'Birman', 'Pug', 'Abyssinian']\n",
            "Unseen classes (15): ['Wheaten Terrier', 'English Setter', 'Keeshond', 'American Pit Bull Terrier', 'Staffordshire Bull Terrier', 'Scottish Terrier', 'Miniature Pinscher', 'Basset Hound', 'Persian', 'Boxer', 'German Shorthaired', 'Great Pyrenees', 'Japanese Chin', 'American Bulldog', 'Bombay']\n"
          ]
        }
      ],
      "source": [
        "all_classes = dataset.classes\n",
        "num_classes = len(all_classes)  # Should be 37\n",
        "\n",
        "# Shuffle deterministically\n",
        "shuffled = all_classes.copy()\n",
        "random.shuffle(shuffled)\n",
        "\n",
        "split_ratio = 0.6\n",
        "split_index = int(num_classes * split_ratio)\n",
        "\n",
        "seen_classes = shuffled[:split_index]\n",
        "unseen_classes = shuffled[split_index:]\n",
        "\n",
        "print(f\"Seen classes ({len(seen_classes)}): {seen_classes}\")\n",
        "print(f\"Unseen classes ({len(unseen_classes)}): {unseen_classes}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxcdPJEZR5H2"
      },
      "source": [
        "Building a filtered dataset - training (seen) and test (unseen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dk4epO1zSAQQ",
        "outputId": "48fbdd8b-c753-40e3-e919-d00331ed34a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size (seen classes): 2184\n",
            "Test dataset size (unseen classes): 1496\n",
            "\n",
            "Training (seen) class idx → breed name:\n",
            "  0 → Bengal\n",
            "  1 → Maine Coon\n",
            "  2 → English Cocker Spaniel\n",
            "  3 → British Shorthair\n",
            "  4 → Newfoundland\n",
            "  5 → Ragdoll\n",
            "  6 → Russian Blue\n",
            "  7 → Beagle\n",
            "  8 → Pomeranian\n",
            "  9 → Samoyed\n",
            "  10 → Sphynx\n",
            "  11 → Shiba Inu\n",
            "  12 → Siamese\n",
            "  13 → Chihuahua\n",
            "  14 → Egyptian Mau\n",
            "  15 → Leonberger\n",
            "  16 → Saint Bernard\n",
            "  17 → Havanese\n",
            "  18 → Yorkshire Terrier\n",
            "  19 → Birman\n",
            "  20 → Pug\n",
            "  21 → Abyssinian\n",
            "\n",
            "Test (unseen) class idx → breed name:\n",
            "  0 → Wheaten Terrier\n",
            "  1 → English Setter\n",
            "  2 → Keeshond\n",
            "  3 → American Pit Bull Terrier\n",
            "  4 → Staffordshire Bull Terrier\n",
            "  5 → Scottish Terrier\n",
            "  6 → Miniature Pinscher\n",
            "  7 → Basset Hound\n",
            "  8 → Persian\n",
            "  9 → Boxer\n",
            "  10 → German Shorthaired\n",
            "  11 → Great Pyrenees\n",
            "  12 → Japanese Chin\n",
            "  13 → American Bulldog\n",
            "  14 → Bombay\n"
          ]
        }
      ],
      "source": [
        "class FilteredOxfordPets(Dataset):\n",
        "    def __init__(self, base_dataset: OxfordIIITPet, allowed_classes: list):\n",
        "        super().__init__()\n",
        "        self.base_dataset = base_dataset\n",
        "        self.allowed_classes = allowed_classes\n",
        "\n",
        "        # Map each allowed class‐name to its original integer index\n",
        "        self.allowed_class_indices = [\n",
        "            base_dataset.classes.index(c) for c in allowed_classes\n",
        "        ]\n",
        "\n",
        "        # Build a list of indices in base_dataset whose label is in allowed_class_indices\n",
        "        self.filtered_indices = [\n",
        "            i\n",
        "            for i, (_, label) in enumerate(base_dataset)\n",
        "            if label in self.allowed_class_indices\n",
        "        ]\n",
        "\n",
        "        # Now, for any image whose original label is `L`,\n",
        "        # we will re‐index it so that L_new = position of L in allowed_class_indices.\n",
        "        # That way, `0 <= L_new < len(allowed_classes)`.\n",
        "        # Example: if allowed_class_indices = [3, 7, 12], then original label=7 → new label=1.\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filtered_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        actual_idx = self.filtered_indices[idx]\n",
        "        img, orig_label = self.base_dataset[actual_idx]\n",
        "        new_label = self.allowed_class_indices.index(orig_label)\n",
        "        return img, new_label\n",
        "\n",
        "# Instantiate train / test splits\n",
        "train_dataset = FilteredOxfordPets(dataset, seen_classes)\n",
        "test_dataset = FilteredOxfordPets(dataset, unseen_classes)\n",
        "\n",
        "print(f\"Train dataset size (seen classes): {len(train_dataset)}\")\n",
        "print(f\"Test dataset size (unseen classes): {len(test_dataset)}\\n\")\n",
        "\n",
        "# Print mapping sanity check\n",
        "print(\"Training (seen) class idx → breed name:\")\n",
        "for new_idx, breed in enumerate(seen_classes):\n",
        "    print(f\"  {new_idx} → {breed}\")\n",
        "print(\"\\nTest (unseen) class idx → breed name:\")\n",
        "for new_idx, breed in enumerate(unseen_classes):\n",
        "    print(f\"  {new_idx} → {breed}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOCo-gH1SSvf"
      },
      "source": [
        "Building class-label embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_HvS21PShI0",
        "outputId": "83e4bbf3-bb61-4487-a3d5-cc5b812e2dc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seen_embeddings shape: torch.Size([22, 384])\n",
            "Unseen_embeddings shape: torch.Size([15, 384])\n",
            "› Saved class_name_embeddings.pt\n"
          ]
        }
      ],
      "source": [
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")  # 384-D output\n",
        "\n",
        "# Option: you could prepend a prompt like \"a photo of a {breed}\" for richer semantics:\n",
        "prompted_seen = [f\"a photo of a {breed}\" for breed in seen_classes]\n",
        "prompted_unseen = [f\"a photo of a {breed}\" for breed in unseen_classes]\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Embed all seen & unseen prompts\n",
        "    seen_emb_list = embedder.encode(prompted_seen, convert_to_tensor=True, normalize_embeddings=True)\n",
        "    unseen_emb_list = embedder.encode(prompted_unseen, convert_to_tensor=True, normalize_embeddings=True)\n",
        "\n",
        "# seen_emb_list: (num_seen, 384), normalized\n",
        "# unseen_emb_list: (num_unseen, 384), normalized\n",
        "\n",
        "seen_embeddings = seen_emb_list.cpu()    # Move to CPU so we can save\n",
        "unseen_embeddings = unseen_emb_list.cpu()\n",
        "\n",
        "print(\"Seen_embeddings shape:\", seen_embeddings.shape)\n",
        "print(\"Unseen_embeddings shape:\", unseen_embeddings.shape)\n",
        "\n",
        "# Save to disk\n",
        "torch.save({\n",
        "    \"seen\": seen_embeddings,\n",
        "    \"unseen\": unseen_embeddings,\n",
        "}, \"class_name_embeddings.pt\")\n",
        "\n",
        "print(\"› Saved class_name_embeddings.pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeSLv7LwSyGx"
      },
      "source": [
        "Building a ResNet18 (predefined CNN) that outputs a 512-D feature vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytDRS-MXS9je"
      },
      "outputs": [],
      "source": [
        "class ResNet18FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        resnet18 = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
        "        # Remove last FC; keep everything up to the global-avgpool\n",
        "        self.features = nn.Sequential(*list(resnet18.children())[:-1])\n",
        "        # The final output after .view(...) will be 512-D\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)           # → (B, 512, 1, 1)\n",
        "        x = x.view(x.size(0), -1)      # → (B, 512)\n",
        "        return x\n",
        "\n",
        "feature_extractor = ResNet18FeatureExtractor().to(device)\n",
        "feature_extractor.eval()  # We won't fine-tune ResNet right now; freeze it\n",
        "for param in feature_extractor.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUJ4nuN6TEEh"
      },
      "source": [
        "Building a mapper: 512 -> 384"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M16t1QcYTHih"
      },
      "outputs": [],
      "source": [
        "class ImageToEmbeddingMapper(nn.Module):\n",
        "    def __init__(self, input_dim=512, output_dim=384):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "mapper = ImageToEmbeddingMapper(input_dim=512, output_dim=384).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biiHw7aRTNHy"
      },
      "source": [
        "DataLoader for train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5shweKCHTPGj",
        "outputId": "0b7904be-8b19-40a3-9f2d-d75c806111c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train_features: torch.Size([2184, 512])\n",
            "Train_labels: torch.Size([2184])\n",
            "Target_embeddings: torch.Size([2184, 384])\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "# Load the previously‐saved class embeddings\n",
        "data = torch.load(\"class_name_embeddings.pt\")\n",
        "seen_embeddings = data[\"seen\"].to(device)    # (num_seen, 384)\n",
        "unseen_embeddings = data[\"unseen\"].to(device)  # (num_unseen, 384)\n",
        "\n",
        "# Collect all training features (512-D) and the corresponding labels\n",
        "all_feats = []\n",
        "all_lbls = []\n",
        "\n",
        "feature_extractor.eval()\n",
        "with torch.no_grad():\n",
        "    for imgs, lbls in train_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        lbls = lbls.to(device)           # each ∈ [0, num_seen−1]\n",
        "\n",
        "        feats = feature_extractor(imgs)  # → (B, 512)\n",
        "        all_feats.append(feats.cpu())\n",
        "        all_lbls.append(lbls.cpu())\n",
        "\n",
        "train_features = torch.cat(all_feats, dim=0)  # (N_seen_examples, 512)\n",
        "train_labels = torch.cat(all_lbls, dim=0)     # (N_seen_examples,)\n",
        "\n",
        "print(\"Train_features:\", train_features.shape)\n",
        "print(\"Train_labels:\", train_labels.shape)\n",
        "\n",
        "# Build target embeddings for each example:\n",
        "#   target_embeddings[i] = seen_embeddings[ train_labels[i] ]\n",
        "target_embeddings = seen_embeddings[train_labels].to(device)  # (N_seen_examples, 384)\n",
        "\n",
        "print(\"Target_embeddings:\", target_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n75hPclTSGS"
      },
      "source": [
        "Setting up training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqkQbh91TbKO",
        "outputId": "3721e3fd-e8fa-4535-bb3f-e663407e706a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100]  Loss: 0.003288\n",
            "Epoch [2/100]  Loss: 0.001994\n",
            "Epoch [3/100]  Loss: 0.001780\n",
            "Epoch [4/100]  Loss: 0.001663\n",
            "Epoch [5/100]  Loss: 0.001573\n",
            "Epoch [6/100]  Loss: 0.001493\n",
            "Epoch [7/100]  Loss: 0.001419\n",
            "Epoch [8/100]  Loss: 0.001349\n",
            "Epoch [9/100]  Loss: 0.001283\n",
            "Epoch [10/100]  Loss: 0.001223\n",
            "Epoch [11/100]  Loss: 0.001170\n",
            "Epoch [12/100]  Loss: 0.001123\n",
            "Epoch [13/100]  Loss: 0.001078\n",
            "Epoch [14/100]  Loss: 0.001039\n",
            "Epoch [15/100]  Loss: 0.001005\n",
            "Epoch [16/100]  Loss: 0.000974\n",
            "Epoch [17/100]  Loss: 0.000943\n",
            "Epoch [18/100]  Loss: 0.000918\n",
            "Epoch [19/100]  Loss: 0.000893\n",
            "Epoch [20/100]  Loss: 0.000871\n",
            "Epoch [21/100]  Loss: 0.000850\n",
            "Epoch [22/100]  Loss: 0.000829\n",
            "Epoch [23/100]  Loss: 0.000810\n",
            "Epoch [24/100]  Loss: 0.000795\n",
            "Epoch [25/100]  Loss: 0.000779\n",
            "Epoch [26/100]  Loss: 0.000764\n",
            "Epoch [27/100]  Loss: 0.000750\n",
            "Epoch [28/100]  Loss: 0.000735\n",
            "Epoch [29/100]  Loss: 0.000723\n",
            "Epoch [30/100]  Loss: 0.000712\n",
            "Epoch [31/100]  Loss: 0.000699\n",
            "Epoch [32/100]  Loss: 0.000688\n",
            "Epoch [33/100]  Loss: 0.000680\n",
            "Epoch [34/100]  Loss: 0.000669\n",
            "Epoch [35/100]  Loss: 0.000660\n",
            "Epoch [36/100]  Loss: 0.000651\n",
            "Epoch [37/100]  Loss: 0.000642\n",
            "Epoch [38/100]  Loss: 0.000635\n",
            "Epoch [39/100]  Loss: 0.000627\n",
            "Epoch [40/100]  Loss: 0.000620\n",
            "Epoch [41/100]  Loss: 0.000613\n",
            "Epoch [42/100]  Loss: 0.000605\n",
            "Epoch [43/100]  Loss: 0.000599\n",
            "Epoch [44/100]  Loss: 0.000592\n",
            "Epoch [45/100]  Loss: 0.000586\n",
            "Epoch [46/100]  Loss: 0.000580\n",
            "Epoch [47/100]  Loss: 0.000577\n",
            "Epoch [48/100]  Loss: 0.000572\n",
            "Epoch [49/100]  Loss: 0.000565\n",
            "Epoch [50/100]  Loss: 0.000560\n",
            "Epoch [51/100]  Loss: 0.000553\n",
            "Epoch [52/100]  Loss: 0.000549\n",
            "Epoch [53/100]  Loss: 0.000546\n",
            "Epoch [54/100]  Loss: 0.000540\n",
            "Epoch [55/100]  Loss: 0.000538\n",
            "Epoch [56/100]  Loss: 0.000532\n",
            "Epoch [57/100]  Loss: 0.000529\n",
            "Epoch [58/100]  Loss: 0.000526\n",
            "Epoch [59/100]  Loss: 0.000520\n",
            "Epoch [60/100]  Loss: 0.000518\n",
            "Epoch [61/100]  Loss: 0.000514\n",
            "Epoch [62/100]  Loss: 0.000508\n",
            "Epoch [63/100]  Loss: 0.000508\n",
            "Epoch [64/100]  Loss: 0.000505\n",
            "Epoch [65/100]  Loss: 0.000507\n",
            "Epoch [66/100]  Loss: 0.000497\n",
            "Epoch [67/100]  Loss: 0.000494\n",
            "Epoch [68/100]  Loss: 0.000495\n",
            "Epoch [69/100]  Loss: 0.000489\n",
            "Epoch [70/100]  Loss: 0.000489\n",
            "Epoch [71/100]  Loss: 0.000486\n",
            "Epoch [72/100]  Loss: 0.000486\n",
            "Epoch [73/100]  Loss: 0.000483\n",
            "Epoch [74/100]  Loss: 0.000480\n",
            "Epoch [75/100]  Loss: 0.000476\n",
            "Epoch [76/100]  Loss: 0.000475\n",
            "Epoch [77/100]  Loss: 0.000472\n",
            "Epoch [78/100]  Loss: 0.000469\n",
            "Epoch [79/100]  Loss: 0.000466\n",
            "Epoch [80/100]  Loss: 0.000463\n",
            "Epoch [81/100]  Loss: 0.000463\n",
            "Epoch [82/100]  Loss: 0.000460\n",
            "Epoch [83/100]  Loss: 0.000461\n",
            "Epoch [84/100]  Loss: 0.000457\n",
            "Epoch [85/100]  Loss: 0.000460\n",
            "Epoch [86/100]  Loss: 0.000456\n",
            "Epoch [87/100]  Loss: 0.000453\n",
            "Epoch [88/100]  Loss: 0.000449\n",
            "Epoch [89/100]  Loss: 0.000448\n",
            "Epoch [90/100]  Loss: 0.000450\n",
            "Epoch [91/100]  Loss: 0.000447\n",
            "Epoch [92/100]  Loss: 0.000444\n",
            "Epoch [93/100]  Loss: 0.000444\n",
            "Epoch [94/100]  Loss: 0.000441\n",
            "Epoch [95/100]  Loss: 0.000448\n",
            "Epoch [96/100]  Loss: 0.000441\n",
            "Epoch [97/100]  Loss: 0.000440\n",
            "Epoch [98/100]  Loss: 0.000438\n",
            "Epoch [99/100]  Loss: 0.000438\n",
            "Epoch [100/100]  Loss: 0.000434\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(mapper.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "EPOCHS = 100\n",
        "batch_size = 64   # We’ll split the big train_features into minibatches\n",
        "\n",
        "# Create a TensorDataset so we can do minibatch SGD on (features, target_embeds)\n",
        "from torch.utils.data import TensorDataset\n",
        "train_tensor_dataset = TensorDataset(train_features, train_labels)\n",
        "# Note: we could also build (train_features, target_embeddings), but re-indexing on the fly:\n",
        "#    inside the loop: tgt = seen_embeddings[ label_batch ]\n",
        "\n",
        "train_tensor_loader = DataLoader(train_tensor_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    mapper.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for feat_batch, lbl_batch in train_tensor_loader:\n",
        "        feat_batch = feat_batch.to(device)       # (B, 512)\n",
        "        lbl_batch = lbl_batch.to(device)         # (B,)\n",
        "\n",
        "        # Forward\n",
        "        pred = mapper(feat_batch)                # (B, 384)\n",
        "        # Normalize pred → (B, 384) to lie on unit sphere (optional but usually helps)\n",
        "        pred_norm = F.normalize(pred, dim=1)\n",
        "\n",
        "        # Look up the target embeddings (already normalized)\n",
        "        tgt = seen_embeddings[lbl_batch]         # (B, 384)\n",
        "\n",
        "        # Compute MSE\n",
        "        loss = criterion(pred_norm, tgt)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * feat_batch.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_tensor_dataset)\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}]  Loss: {epoch_loss:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXmlsdbVTkpc"
      },
      "source": [
        "DataLoader for test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5Mb6MUzTnC0",
        "outputId": "e97eb10b-f9c9-43c6-a563-fea2c85e726c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✔ True: American Bulldog     | Predicted: Miniature Pinscher\n",
            "✔ True: American Bulldog     | Predicted: Scottish Terrier\n",
            "✔ True: American Bulldog     | Predicted: Staffordshire Bull Terrier\n",
            "✔ True: American Bulldog     | Predicted: Basset Hound\n",
            "✔ True: American Bulldog     | Predicted: Scottish Terrier\n",
            "✔ True: American Bulldog     | Predicted: Scottish Terrier\n",
            "✔ True: American Bulldog     | Predicted: Scottish Terrier\n",
            "✔ True: American Bulldog     | Predicted: Scottish Terrier\n",
            "✔ True: American Bulldog     | Predicted: Basset Hound\n",
            "✔ True: American Bulldog     | Predicted: Basset Hound\n",
            "✔ True: American Bulldog     | Predicted: Great Pyrenees\n",
            "✔ True: American Bulldog     | Predicted: Scottish Terrier\n",
            "✔ True: American Bulldog     | Predicted: Basset Hound\n",
            "✔ True: American Bulldog     | Predicted: Basset Hound\n",
            "✔ True: American Bulldog     | Predicted: Basset Hound\n",
            "✔ True: American Bulldog     | Predicted: Scottish Terrier\n",
            "✔ True: American Bulldog     | Predicted: Basset Hound\n",
            "✔ True: American Bulldog     | Predicted: German Shorthaired\n",
            "✔ True: American Bulldog     | Predicted: Scottish Terrier\n",
            "✔ True: American Bulldog     | Predicted: Basset Hound\n",
            "\n",
            "🔍 Zero-Shot Accuracy on Unseen Classes: 15.57%\n"
          ]
        }
      ],
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "mapper.eval()\n",
        "feature_extractor.eval()\n",
        "\n",
        "total = 0\n",
        "correct = 0\n",
        "\n",
        "# To turn predicted index → breed name:\n",
        "idx2unseen = {i: breed for i, breed in enumerate(unseen_classes)}\n",
        "\n",
        "# For optional sanity printing:\n",
        "MAX_PRINT = 20\n",
        "printed = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, true_lbls in test_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        true_lbls = true_lbls.to(device)   # each ∈ [0, num_unseen−1]\n",
        "\n",
        "        feats = feature_extractor(imgs)    # → (B, 512)\n",
        "        preds_raw = mapper(feats)          # → (B, 384)\n",
        "        preds_norm = F.normalize(preds_raw, dim=1)  # normalize to unit sphere\n",
        "\n",
        "        # Normalize unseen_embeddings if not already:\n",
        "        unseen_norm = F.normalize(unseen_embeddings.to(device), dim=1)  # (num_unseen, 384)\n",
        "\n",
        "        # Cosine similarities: (B, 384) × (384, num_unseen) → (B, num_unseen)\n",
        "        sims = torch.matmul(preds_norm, unseen_norm.T)\n",
        "\n",
        "        # Pick top index along unseen side\n",
        "        pred_indices = sims.argmax(dim=1)  # (B,)\n",
        "\n",
        "        # Compute accuracy\n",
        "        correct += (pred_indices == true_lbls).sum().item()\n",
        "        total += true_lbls.size(0)\n",
        "\n",
        "        # (Optional) print a few examples\n",
        "        if printed < MAX_PRINT:\n",
        "            for t_lbl, p_lbl in zip(true_lbls, pred_indices):\n",
        "                true_breed = idx2unseen[t_lbl.item()]\n",
        "                pred_breed = idx2unseen[p_lbl.item()]\n",
        "                print(f\"✔ True: {true_breed:20s} | Predicted: {pred_breed}\")\n",
        "                printed += 1\n",
        "                if printed >= MAX_PRINT:\n",
        "                    break\n",
        "\n",
        "accuracy = 100.0 * correct / total\n",
        "print(\"\\n🔍 Zero-Shot Accuracy on Unseen Classes: {:.2f}%\".format(accuracy))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}